[project]
name = "oura-ring-parser"
version = "0.1.0"
description = "Oura Ring Gen 3 reverse engineering - native parser, ML inference, and BLE client"
requires-python = ">=3.11"
license = { text = "MIT" }

# Default dependencies (web backend + BLE)
dependencies = [
    "protobuf>=4.0.0",
    "numpy>=1.24.0",
    # Web backend
    "fastapi>=0.100.0",
    "uvicorn[standard]>=0.22.0",
    "pydantic>=2.0.0",
    "python-multipart>=0.0.6",
    "websockets>=12.0",
    "aiofiles>=23.0.0",
    "httpx>=0.25.0",
    # BLE client
    "bleak>=0.21.0",
    "pycryptodome>=3.15.0",
]

[project.optional-dependencies]
# ML inference (PyTorch CPU) - only needed if running ML locally instead of Docker
ml = [
    "torch>=2.0.0",
]

# Development tools
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
]

[project.scripts]
# BLE CLI entry point
oura-ble = "oura.ble.cli:cli_main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["native_parser/oura", "native_parser/ml_inference"]

[tool.uv]
# Use CPU-only PyTorch index for smaller downloads
extra-index-url = ["https://download.pytorch.org/whl/cpu"]

[tool.ruff]
line-length = 100
target-version = "py311"

[tool.pytest.ini_options]
asyncio_mode = "auto"
